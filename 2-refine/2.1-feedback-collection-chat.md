# 2.1.1: Feedback Collection

[⬅️ Day 2 Overview](README.md)

## 2.1 Feedback Collection

**Goal**: Create a `feedback-analysis.md` file based on REAL user feedback from actual conversations with your target users.

**Process**: Talk to at least 5 potential users from your target audience. Don't just imagine what users might say - GET OUT OF THE BUILDING and talk to real people. Then follow this guide with Claude.ai to organize and analyze what you learned.

**Timeframe**: 2-3 hours (1 hour for user conversations, 1 hour for analysis)

## How to Conduct Effective User Demos & Interviews

Before you start collecting feedback, you need to know how to conduct effective user demos and interviews the YC way:

1. **Find the right people**: Talk ONLY to your actual target users. Friends and family will be too nice - find strangers in your target market. Use social media, online communities, or cold outreach.

2. **Set proper expectations**: Tell them "This is an early prototype. It will have issues. Please be brutally honest - you help me most by telling me what's wrong."

3. **Shut up and listen**: Show them the prototype, give minimal instructions, then BE QUIET. Watch where they get confused without jumping in to explain.

4. **Ask open-ended questions**: 
   - "What was confusing about this process?"
   - "What did you expect to happen when you clicked that?"
   - "What would make you pay for this?"
   - "What's the most frustrating part of using this?"
   - NEVER: "Did you like it?" or "Was that easy?"

5. **Dig deeper on problems**: When they mention issues, ask "Can you tell me more about that?" or "Why was that frustrating?" Get to root causes.

6. **End with key questions**:
   - "Would you use this again? Why or why not?"
   - "Would you be disappointed if this product disappeared tomorrow?"
   - "What would make you recommend this to a colleague/friend?"

7. **Watch what they DO, not just what they SAY**: Note where they hesitate, look confused, or try clicking things that aren't clickable.

Now proceed with these steps to analyze what you learn:

### 2.1.1: Raw Feedback Collection

```
I've built a prototype for my SaaS application [your app name] which [briefly describe core functionality].

I've demoed it to [number] potential users in my target market of [describe target users] and collected their direct feedback. Here's what they actually said and did:

[List REAL quotes and observations from your user demos. Be specific and include both positive and negative feedback. Include at least 5-7 distinct points of feedback.]

For example:
- User A said: "I couldn't figure out how to start a new project - the '+' button wasn't obvious to me"
- User B struggled for 30 seconds trying to find where to access their saved work
- User C said: "I really liked how fast it generated results, but I was confused by some of the terminology"
- I noticed 3/5 users tried clicking on the header image expecting it to do something

Help me organize this feedback systematically by:
1. Categorizing comments by type (UI/UX, functionality, performance, etc.)
2. Identifying common themes or patterns in the feedback
3. Highlighting which issues are preventing users from experiencing the core value
4. Noting which pieces of feedback contradict each other (if any)
```

### 2.1.2: Prioritization Matrix

```
Based on this categorized feedback, help me create a ruthlessly prioritized list of what to fix first.

Create an impact vs. effort matrix with these specific guidelines:
1. HIGH IMPACT = directly prevents users from experiencing the core value proposition
2. HIGH EFFORT = would take more than 1 day to implement

Then recommend:
1. The 3 most critical fixes I should make immediately (should all be high impact, low effort)
2. Any "deal-breaker" issues that MUST be fixed even if they're high effort
3. Low impact issues I should explicitly ignore for now to maintain focus

Remember that I need to focus ONLY on what will help me get users who would be disappointed if they couldn't use my product tomorrow. Everything else is a distraction at this stage.
```

### 2.1.3: Competition and Differentiation

```
During my user demos, I asked users about alternatives they currently use. Here's what I learned:

[List 2-3 competitors your users mentioned and what they said about them]

For each competitor, note:
- What users like better about the competitor
- What users like better about your solution
- The key pain points users still have with existing solutions

Based on this feedback, help me identify:
1. My ACTUAL unique value proposition (not what I think it is, but what users confirmed)
2. The 2-3 areas where I MUST be better than alternatives to win customers
3. Features I can ignore because users don't actually care about them (even if competitors have them)
```

### 2.1.4: Observed User Behavior

```
Beyond what users told me, here's what I observed when watching them use the prototype:

[List 3-5 specific behaviors you observed during demos]

For example:
- 4/5 users hesitated for >5 seconds when they landed on the dashboard
- 3/5 users tried to drag-and-drop items instead of using the menu
- All users scrolled past the help section without reading it
- 2/5 users verbally expressed frustration when trying to complete [specific task]

For each observation, help me:
1. Identify the specific UX/UI issue causing this behavior
2. Suggest a concrete, minimal change to fix it
3. Define how I'll know if the fix worked (what user behavior should change)
```

### 2.1.5: Feedback Analysis Document

```
Based on all the real user feedback we've discussed, create a focused, actionable feedback analysis document with:

1. Executive Summary (3 bullet points maximum)
   - The ONE thing users valued most
   - The TOP problem preventing adoption
   - The CRITICAL differentiator from competitors

2. Prioritized Action Items
   - 3-5 specific, concrete changes to make immediately
   - What success looks like for each change
   - Order of implementation

3. User-Validated Strengths
   - Features/aspects users explicitly valued (with actual quotes)
   - Confirmed differentiation points from alternatives
   - Elements that should NOT be changed

4. Next Round Testing Plan
   - Specific hypotheses to test in the next iteration
   - Questions to ask in the next user interviews
   - How to measure improvement

Format this as a clean, minimal markdown document I can save as feedback-analysis.md and reference while making changes.

The document should be ruthlessly focused on actionable insights, not comprehensive analysis. Every item should directly connect to either retaining users or acquiring new ones.
```

### 2.1.6: Follow Up Plan

```
Based on this feedback analysis, create a concrete step-by-step plan to:

1. Fix the top 3 critical issues
2. Enhance the 1-2 strongest differentiators
3. Test with at least 5 NEW users from my target audience
4. Measure whether the changes actually improved the key metrics

For each step, specify:
- The exact change to implement
- How long it should take
- How to validate it worked

The goal is to create a significantly improved version that I can demo to new users. Time is running out - we need to get to product-market fit.

Output the final `feedback-analysis.md` file that encompasses everything I learned, discussed, and planned. 
```