# Day 1: Create

[⬅️ Day 1 Overview](README.md)

## 1.4 Database Setup Introduction

**Goal**: Define the data structure (schema) required for the web application, creating a set of migration files that will fully install a custom Postgres database into your Supabase instance. Also create a `seed.sql` file to populate an initial dataset.

**Process**: Follow this chat pattern with an AI chat tool such as [Claude.ai](https://www.claude.ai). Pay attention to the notes in `[brackets]` and replace the brackets with your own thoughts and ideas.

**Timeframe**: 30-45 minutes

## Table of Contents
- [1.4.1: Initial Schema Analysis](#141-initial-schema-analysis)
- [1.4.2: Foundation Migration](#142-foundation-migration)
- [1.4.3: Structure Migration](#143-structure-migration)
- [1.4.4: Relationships Migration](#144-relationships-migration)
- [1.4.5: Security Migration](#145-security-migration)
- [1.4.6: Seed Data](#146-seed-data)
- [1.4.7: Schema Validation](#147-schema-validation)
- [1.4.8: Schema Refinement (Optional)](#148-schema-refinement-optional)
- [1.4.9: Installation Instructions](#149-installation-instructions)

Have this link open and ready to copy/paste in: [supabase-database-patterns-new.md](../0-learn/supabase-database-patterns-new.md)

## 1.4 Database Setup Generation

### 1.4.1: Initial Schema Analysis

```
You are a Supabase and Postgres and SQL expert tasked with helping an entrepreneur turn product requirements into a well-structured Supabase database. You'll help analyze requirements and create properly organized migration files.

Migration Overview: We are starting with initial schema analysis only. This step focuses on understanding our database requirements without writing any SQL code yet. Later steps will handle the actual migration files (foundation, structure, relationships, and security).

Please paste your completed `product-requirements.md` document from step 1.1 and the `supabase-database-patterns-new.md` file below:

<product-requirements>
[Paste your PRD here]
</product-requirements>

<supabase-database-patterns>
[Paste the content from the supabase-database-patterns-new.md file linked to above.]
</supabase-database-patterns>

Supabase has reserved schemas with special purposes that can be made use of when it makes sense (see supabase-database-patterns). My custom schemas should be organized as:
- `analytics` - Reporting data (primarily views)
- `api` - User-generated content and application data
- `config` - Application configuration data
- `reference` - Publicly available lookup tables
- `stripe` - Synced data from Stripe webhooks (if using Stripe)

Based on these documents, please analyze the core entities needed for your database schema:

1. The schema organization:
   - What custom schemas should we create?
   - What types of data should go in each schema?
   - What enums should we place in the reference schema?
   - Should we create domain types for common patterns (like email, url)?

2. Core business objects specific to your application:
   - What are the main entities that represent your application's unique value?
   - How do these entities relate to users?
   - What timestamps and audit fields should be included?
   - Should we implement soft deletes with deleted_at timestamps?

3. Profile and authentication extensions:
   - How should we extend the default auth.users table?
   - What additional user-related data do we need to store?
   - How should we handle user roles and permissions?

4. Subscription and billing tables (if applicable):
   - Do we need to implement Stripe integration tables?
   - What customizations do we need for your pricing model?
   - How will we sync subscription data between Stripe and our application?

5. Usage tracking and analytics requirements:
   - What usage metrics do we need to track?
   - How will this data be structured for reporting?
   - Do we need any materialized views for analytics?

For each entity, please identify:
- Key attributes/columns
- Primary identifiers (UUIDs vs serial IDs)
- Basic relationships to other entities
- Access patterns (who needs to read/write this data)

Don't write SQL yet - just create an initial analysis of entities, relationships, and key security/access patterns.
```

### 1.4.2: Foundation Migration

```
Thank you for the initial analysis. Now let's create our first migration file: `000-foundation.sql` which will establish the foundation of our database structure.

Migration Overview: We are at step 1 of 4 in our migration process. This first migration (000-foundation.sql) will ONLY set up extensions, schemas, and basic types. Later migrations will handle tables, relationships, and security policies.

When creating database schemas, follow these best practices:
- Use UUIDs as primary keys for better distribution and security
- Include created_at and updated_at timestamps on all tables
- Use foreign key constraints to maintain referential integrity
- Consider soft deletes with a deleted_at timestamp instead of hard deletes

In this migration, we'll include:

1. EXTENSIONS
   - Enable all required PostgreSQL extensions for MVP features (pgcrypto, pg_graphql, etc.)

2. SCHEMAS
   - Create all custom schemas needed for our application
   - Document the purpose of each schema

3. TYPES AND DOMAINS
   - Define all enums and custom types
   - Create domain types if needed (email, url, etc.)
   - Place enums in the reference schema

Please generate the complete 000-foundation.sql file based on our application requirements.
```

**Optional**: Additional considerations

```
I'd like your perspective on:

1. Should we implement soft deletes (using deleted_at) for any tables?
   [Provide answer]

2. What timestamp fields should we include beyond created_at and updated_at?
   [Provide answer]

3. Should we use UUIDs for all primary keys or are there exceptions?
   [Provide answer]
```

### 1.4.3: Structure Migration

```
Great! Now let's create our second migration file: `001-structure.sql` which will define the core data structures of our application.

Migration Overview: We are at step 2 of 4 in our migration process. This second migration (001-structure.sql) focuses ONLY on creating utility functions, reference tables, and core tables. We'll handle relationships, foreign keys, and security in later migrations.

This migration will include:

1. UTILITY FUNCTIONS
   - Create reusable utility functions that will be used by triggers and policies
   - Follow security best practices for function definitions:
     - Set explicit search paths to prevent injection (SET search_path = public, pg_temp)
     - Use appropriate security context (SECURITY DEFINER vs INVOKER)
     - Include proper error handling

2. REFERENCE TABLES
   - Create lookup/reference tables in the reference schema
   - These tables should contain static data that rarely changes
   - Include appropriate constraints and descriptions

3. CORE TABLES
   - Define main business entity tables
   - For each table, include:
     - Primary key (using UUIDs)
     - Required columns with appropriate data types
     - Default values where applicable
     - NOT NULL constraints
     - CHECK constraints for data validation
     - Timestamp fields (created_at, updated_at)
     - Description of each column's purpose

For user profiles, follow this pattern:
- Store extended user data in api.profiles, not directly in auth.users
- Link profiles to auth users with a foreign key reference

For all tables, consider:
- Who can create records
- Who can view records
- Who can update records
- Who can delete records

Please generate the complete 001-structure.sql file.
```

**Optional**: Advanced feedback questions

```
Please consider the following questions:

1. How will we handle multi-tenant data isolation in our schema?
   [Provide answer]

2. Do we need any special tables for admin users or special roles?
   [Provide answer]

3. Should we implement any version tracking for key entities?
   [Provide answer]
```

### 1.4.4: Relationships Migration

```
Now let's create our third migration file: `002-relationships.sql` which will establish the relationships between our tables and optimize query performance.

Migration Overview: We are at step 3 of 4 in our migration process. This third migration (002-relationships.sql) focuses ONLY on defining relationships between tables, creating indexes, and setting up views. Security and RLS policies will be handled in the next migration.

This migration will include:

1. RELATIONSHIPS AND FOREIGN KEYS
   - Define all relationships between tables
   - Specify foreign key constraints with appropriate actions:
     - ON DELETE (CASCADE, SET NULL, RESTRICT)
     - ON UPDATE (CASCADE, RESTRICT)
   - Add unique constraints or composite keys where needed
   - Implement check constraints to enforce business rules

2. INDEXES
   - Create indexes for:
     - Foreign keys that will be frequently queried
     - Columns often used in WHERE clauses
     - Columns used for sorting (ORDER BY)
     - Consider composite indexes for multi-column queries
     - Add GIN indexes for JSONB or array columns if needed
   - Remember that each index:
     - Speeds up read operations
     - Slows down write operations
     - Increases storage requirements

3. VIEWS
   - Create views for:
     - Common complex queries
     - Reporting and analytics
     - Joining data across schemas
   - Consider materialized views only for complex analytics that are infrequently updated

For foreign keys, follow this pattern:
- Name constraints clearly (e.g., fk_posts_user_id)
- Consider the impact of cascading deletes
- Index foreign key columns

Please generate the complete 002-relationships.sql file.
```

**Optional**: Advanced feedback questions

```
Consider the following questions:

1. How should we handle cascading deletes for user-owned data?
   [Provide answer]

2. Are there any many-to-many relationships requiring junction tables?
   [Provide answer]

3. Which foreign keys will be frequently queried and need indexes?
   [Provide answer]

```

### 1.4.5: Security Migration

```
Now let's create our final migration file: `003-security.sql` which will implement security, automation, and API functions.

Migration Overview: We are at step 4 of 4 in our migration process. This final migration (003-security.sql) focuses ONLY on implementing triggers, RLS policies, storage configuration, and custom API functions to secure our data and automate processes.

This migration will include:

1. TRIGGERS
   - Create triggers for:
     - Updating timestamp fields (updated_at)
     - Creating related records automatically
     - Enforcing business rules
     - Syncing data between tables
   - Follow these best practices:
     - Use proper schema qualification for function references
     - Create explicit trigger statements (avoid dynamic SQL)
     - Test trigger behavior thoroughly
     - Handle the NEW record properly in trigger functions

2. RLS POLICIES
   - Enable Row Level Security on all tables with user data
   - Implement policies for each operation type:
     - FOR SELECT - who can view which records
     - FOR INSERT - who can create records
     - FOR UPDATE - who can modify which records
     - FOR DELETE - who can remove which records
   - Use common policy patterns:
     - Ownership-based access (users see only their data)
     - Role-based access (permissions based on user roles)
     - Relationship-based access (team members can see team data)

3. STORAGE CONFIGURATION
   - Configure storage buckets based on access patterns:
     - public - Openly accessible files
     - protected - Authenticated-only access
     - private - User-specific private files
   - Apply RLS policies to storage buckets to control file access
   - Always fully qualify column names to avoid ambiguity

4. CUSTOM API FUNCTIONS
   - Define database functions for complex operations
   - Ensure each function:
     - Has proper security context
     - Sets explicit search paths
     - Includes error handling
     - Returns appropriate types

For RLS policies, always be aware that:
- Tables with RLS enabled but no policies block all access by default
- Policies should be created before or immediately after enabling RLS

Please generate the complete 003-security.sql file.
```

**Optional**: Advanced feedback questions

```
Consider the following questions:

1. Do we need any service role bypass mechanisms for background processing?
   [Provide answer]

2. Should we implement triggers for maintaining audit history?
   [Provide answer]

3. Do we need any specialized billing or subscription functions?
   [Provide answer]

4. How should we handle multi-tenant data isolation in our security policies?
   [Provide answer]
```

### 1.4.6: Seed Data

```
Now that we have our database schema defined, let's create a seed.sql file to populate our reference tables with essential data.

Migration Overview: All migration files are now complete. We're now creating a separate seed.sql file to populate reference tables with initial data. This is not a migration file but will be used after migrations to provide essential lookup data.

For seed data, focus exclusively on:
1. Populating reference tables with required values
2. Including standard lookup data the application needs to function
3. NOT inserting any user data or user-generated content
4. Providing only minimal reference data needed for operation

Examples of appropriate seed data include:
- Status options and types
- Category definitions
- Configuration settings
- Role definitions
- Permission types
- Standard options for dropdown menus
- Other static lookup values

For each reference table:
- Add a comment explaining what data is being inserted
- Use INSERT statements with appropriate values
- Provide consistent IDs for stable references

Please generate a complete seed.sql file we can save as 'supabase/seed.sql'.
```

### 1.4.7: Schema Validation

```
Now that we've created our migration files and seed data, let's validate our schema against the original requirements and the Supabase database patterns.

Migration Overview: All migrations and seed data are complete. We're now validating our schema against requirements to ensure we haven't missed anything before implementation. No new SQL code will be generated in this step.

Please review our database design and compare it to both the original product requirements and database patterns:

1. Completeness check:
   - Are all the key entities from the product requirements represented?
   - Have we implemented all necessary relationships?
   - Does the schema support all the main user flows described in the requirements?
   - Is there anything missing from the core business objects?

2. Pattern implementation:
   - Have we correctly organized our schemas according to the patterns?
   - Are we using proper authentication and profile patterns?
   - Have we correctly implemented the subscription tables (if needed)?
   - Are we following the security best practices for RLS policies?
   - Do all our functions properly set search paths and security contexts?

3. Identify any gaps or misalignments:
   - Are there any requirements not fully supported by this schema?
   - Did we make any design compromises that should be documented?
   - Are there any areas where we diverged from the patterns, and if so, why?
   - Do we need any additional tables or relationships for future requirements?

4. Security assessment:
   - Have we enabled RLS on all user-data tables?
   - Do all sensitive tables have appropriate policies?
   - Have we handled storage security properly?
   - Are there any potential security holes in our design?

5. Scalability and performance considerations:
   - Will this schema design scale with expected user growth?
   - Have we identified potential performance bottlenecks?
   - Are our indexing strategies appropriate for expected query patterns?
   - Have we avoided premature optimization?

6. Database maintenance:
   - Have we implemented proper triggers for timestamps and auditing?
   - Are our foreign key constraints appropriate?
   - Is our schema organized logically for future modifications?

Please provide a thoughtful analysis highlighting both the strengths of our schema design and areas that might need further refinement before final implementation.
```

### 1.4.8: Schema Refinement (Optional)

```
Based on our schema validation, I'd like to make some refinements before finalizing:

Migration Overview: This optional step allows for refinements to any of our previously created migration files based on the validation results. We may modify any of the four migration files or the seed data as needed.

1. [Add any concerns about the proposed schema]
2. [Mention any missing entities or attributes]
3. [Note any performance or scaling considerations]
4. [Suggest any simplifications or optimizations]

Please review these concerns and suggest specific modifications to our migration files to address these issues:

1. How should we modify our schema to address [concern 1]?
2. What additional tables or columns do we need for [concern 2]?
3. What indexing or optimization strategy do you recommend for [concern 3]?
4. How can we simplify [concern 4] while maintaining functionality?

Please provide specific SQL statements or modifications we should make to our migration files.
```

### 1.4.9: Installation Instructions

After generating your migration files and seed data, here's how to install them:

**Using the Supabase Dashboard:**
1. Go to your Supabase project dashboard
2. Navigate to the SQL Editor
3. Create a new query for each migration file
4. Run the migrations in order (000, 001, 002, 003)
5. Run the seed.sql file last
6. If you encounter any errors:
   - Copy the complete error message
   - Paste it back to Claude with a request to fix the issue
   - Apply the corrected SQL script

**Using the Supabase CLI locally:**
1. Save each migration file in the `supabase/migrations/` directory following this naming pattern:
   - `20230101000000_foundation.sql`
   - `20230101000001_structure.sql`
   - `20230101000002_relationships.sql`
   - `20230101000003_security.sql`
2. Save the seed script as `supabase/seed.sql`
3. Run `supabase db reset` to apply all migrations and seed data
4. This gives you a fresh database with schema and test data each time

For your production environment:
- Apply seed data only for essential reference tables
- Remove any test accounts before deployment

After running these, your database will be fully set up with proper security policies, optimizations, and initial data for your application.